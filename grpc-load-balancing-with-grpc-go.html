<!DOCTYPE html><html><head><meta charSet="utf-8"/><link rel="preconnect" href="https://www.google-analytics.com"/><link rel="preload" href="https://fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic&amp;display=optional" as="style"/><title>gRPC load balancing with grpc-go</title><meta name="description" content="Rafael Eyng&#x27;s tech blog"/><meta name="language" content="en"/><meta name="content-language" content="en"/><meta name="author" content="Rafael Eyng"/><meta name="keywords" content="software, development, javascript, github, node, docker, blog"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="canonical" href="https://rafaeleyng.github.io/grpc-load-balancing-with-grpc-go"/><link rel="shortcut icon" href="/favicon.png" type="image/x-icon"/><link href="https://fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic&amp;display=optional" rel="stylesheet" type="text/css"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-154633858-1"></script><script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-154633858-1');
</script><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/e83c54905e1409242ee1a2330441cd232091e570_CSS.7b3f6175.chunk.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e83c54905e1409242ee1a2330441cd232091e570_CSS.7b3f6175.chunk.css"/><link rel="preload" href="/_next/static/E5zKtSWyHCoyngCKK54Qr/pages/post.js" as="script"/><link rel="preload" href="/_next/static/E5zKtSWyHCoyngCKK54Qr/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-9369c5c69dbf6d4912cb.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.03885e3762b811c06d48.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.2d46a4062a63030a7155.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-fd16d9fd7045bb52879a.js" as="script"/><link rel="preload" href="/_next/static/chunks/a1bc03cf.5784ec4ab76361fe4054.js" as="script"/><link rel="preload" href="/_next/static/chunks/e83c54905e1409242ee1a2330441cd232091e570.751c034d2cba01f871b4.js" as="script"/><link rel="preload" href="/_next/static/chunks/e83c54905e1409242ee1a2330441cd232091e570_CSS.c4a6afef92e8dd991da1.js" as="script"/></head><body><div id="__next"><div class="container"><header class="site-header single-column"><a class="blog-title" href="/">Rafael Eyng&#x27;s Blog</a><nav class="blog-menu"><a target="_blank" rel="noopener" href="https://rafaeleyng.github.io/me">About</a><a target="_blank" rel="noopener" href="https://github.com/rafaeleyng/rafaeleyng.github.io">Github</a></nav></header></div><header class="page-header"><div class="container single-column"><h1>gRPC load balancing with grpc-go</h1><p class="post-meta"><time itemProp="datePublished" dateTime="2020-11-21">Nov 21, 2020</time></p></div></header><main class="container single-column"><section class="post"><span class="hidden" itemProp="publisher">Rafael Eyng</span><span class="hidden" itemProp="keywords">grpc, load balancing, golang, kubernetes</span><article class="post-content" itemProp="articleBody"><p>gRPC poses a <a href="https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/">known problem</a> for load balancing if you have an L4 load balancer in front of multiple instances of your backend gRPC server.</p><p>In short, L4 load balancers balanced at the <em>connection level</em>, which for HTTP 1.1 normally is just fine. But gRPC uses HTTP 2, where a single, long-lived connection is kept and all requests are multiplexed within it. So we would need a balancer working at the <em>request level</em>.</p><p>My team recently faced that issue, and we used an L4 balancer, in the form of a Kubernetes external service (<code>type: LoadBalancer</code>). Since changing that was not an option at the moment, we took the approach of client-side balancing, which itself was hard to set up because the documentation was somewhat lacking. We also considered using a <a href="https://grpc.io/blog/grpc-load-balancing/#lookaside-load-balancing">look-aside</a> load balancer, but the client-side ended up being easier to implement and maintain.</p><h2>Constraints</h2><p>We needed a solution that would work well with several constraints since our system runs in a dynamic environment, where the gRPC server instances are not expected to be statically known:
1. the client must to discover all the instances of the gRPC server and open a single, long-lived connection directly with each one (not going through the load balancer)
3. if instances of the gRPC server are removed, the client must acknowledge that and remove those connections
2. similarly, if new instances of the gRPC server are added, the client must create new connections with those new instances</p><h2>DNS name resolution</h2><p>The gRPC documentation mentions support for <a href="https://github.com/grpc/grpc/blob/master/doc/naming.md">DNS as the default name system</a>. It wasn&#x27;t obvious to me at first how to benefit from this, but our co-workers who work on <a href="https://github.com/tsuru/tsuru">Tsuru</a> (our PaaS that works on top of Kubernetes) suggested using a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Service</a> as a way of obtaining the addresses of the Pods behind our actual Service.</p><p>We used the lib go-grpc, which we&#x27;ve found to have support for <a href="https://github.com/grpc/grpc-go/blob/master/internal/resolver/dns/dns_resolver.go">DNS resolver</a> and also for balancing requests across several instances with various strategies (we went for <a href="https://github.com/grpc/grpc-go/blob/master/balancer/roundrobin/roundrobin.go">round-robin</a>).</p><p>Configuring this was not clearly documented in lib as would I expect. The two main changes we&#x27;ve done in our client were:</p><ul><li>add the <code>WithDefaultServiceConfig</code> DialOption with the load balancing policy</li><li>specify a DNS URI pointing to the Headless Server we&#x27;ve mentioned</li></ul><p>The result was like:</p><pre><code class="language-go">import (
  &quot;google.golang.org/grpc&quot;
)

conn, err := grpc.Dial(&quot;dns:///my-headless-service:5000&quot;,
  grpc.WithDefaultServiceConfig(`{&quot;loadBalancingPolicy&quot;:&quot;round_robin&quot;}`),
)</code></pre><p>Since I could not access the Headless Service from my local machine in development (it is an intra-cluster address), I had to set up a local DNS server to experiment with this, while on production I would not specify a DNS and let the Pod use its default.</p><p>Never having worked with it before, the DNS URI tricked me into some errors. First, <a href="https://github.com/grpc/grpc/blob/master/doc/naming.md">the documentation</a> states that the scheme is <code>dns:[//authority/]host[:port]</code>, which makes all slashes lookup optional when you do not specify an authority (the DNS server you want to use, if not the default configured on the OS), which was my case in production.</p><p>So while I was able to make this work locally by using <code>dns://localhost:1053/my-headless-service:5000</code>, in production I first tried a naked <code>dns:my-headless-service:5000</code> and a double-slashed <code>dns://my-headless-service:5000</code> before landing on the correct triple-slashed <code>dns:///my-headless-service:5000</code>.</p><p>Also, note that the port <code>5000</code> is not important in this resolution process. It is just the port where each pod is exposing my gRPC server.</p><h2>Connection Timeout on gRPC server</h2><p>Remember my 3 constraints? The client configuration only solves number 1.</p><p>If I remove instances of my service, it would cause connections to fail and <a href="https://github.com/grpc/grpc-go/issues/3170#issuecomment-552517779">the client to re-resolve the names</a>. This is default behavior of the lib and solves constraint number 2.</p><p>If everything is stable, the client never re-resolves the names and recreates new connections. So, if I double the number of instances of my service, the new ones would never receive connections and would be idle. And this fails constraint 3.</p><p>To work around this, we&#x27;ve configured a <a href="https://github.com/grpc/proposal/blob/master/A9-server-side-conn-mgt.md">MAX_CONNECTION_AGE</a>:</p><pre><code class="language-go">import (
  &quot;google.golang.org/grpc&quot;
  &quot;google.golang.org/grpc/keepalive&quot;
)

opts := []grpc.ServerOption{
  grpc.KeepaliveParams(keepalive.ServerParameters{
  MaxConnectionAge: time.Minute * 5,
  }),
}</code></pre><p>When the connection reaches its max-age, it will be closed and will trigger a re-resolve from the client. If new instances were added in the meantime, the client will see them now and open connections to them as well.</p><h2>Demo</h2><p>I&#x27;ve set up <a href="https://github.com/rafaeleyng/rafaeleyng.github.io/blob/dev/examples/grpc-load-balancing/README.md">a demo</a> with a local DNS server to show this working.</p></article><div id="disqus_thread"></div><script type="text/javascript">
var disqus_config = function () {
  this.page.url = 'https://rafaeleyng.github.io/grpc-load-balancing-with-grpc-go';
  this.page.identifier = '/grpc-load-balancing-with-grpc-go';
};

(function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://rafaeleyng.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript></section></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"document":{"content":"\ngRPC poses a [known problem](https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/) for load balancing if you have an L4 load balancer in front of multiple instances of your backend gRPC server.\n\nIn short, L4 load balancers balanced at the _connection level_, which for HTTP 1.1 normally is just fine. But gRPC uses HTTP 2, where a single, long-lived connection is kept and all requests are multiplexed within it. So we would need a balancer working at the _request level_.\n\nMy team recently faced that issue, and we used an L4 balancer, in the form of a Kubernetes external service (`type: LoadBalancer`). Since changing that was not an option at the moment, we took the approach of client-side balancing, which itself was hard to set up because the documentation was somewhat lacking. We also considered using a [look-aside](https://grpc.io/blog/grpc-load-balancing/#lookaside-load-balancing) load balancer, but the client-side ended up being easier to implement and maintain.\n\n## Constraints\n\nWe needed a solution that would work well with several constraints since our system runs in a dynamic environment, where the gRPC server instances are not expected to be statically known:\n1. the client must to discover all the instances of the gRPC server and open a single, long-lived connection directly with each one (not going through the load balancer)\n3. if instances of the gRPC server are removed, the client must acknowledge that and remove those connections\n2. similarly, if new instances of the gRPC server are added, the client must create new connections with those new instances\n\n## DNS name resolution\n\nThe gRPC documentation mentions support for [DNS as the default name system](https://github.com/grpc/grpc/blob/master/doc/naming.md). It wasn't obvious to me at first how to benefit from this, but our co-workers who work on [Tsuru](https://github.com/tsuru/tsuru) (our PaaS that works on top of Kubernetes) suggested using a [Headless Service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) as a way of obtaining the addresses of the Pods behind our actual Service.\n\nWe used the lib go-grpc, which we've found to have support for [DNS resolver](https://github.com/grpc/grpc-go/blob/master/internal/resolver/dns/dns_resolver.go) and also for balancing requests across several instances with various strategies (we went for [round-robin](https://github.com/grpc/grpc-go/blob/master/balancer/roundrobin/roundrobin.go)).\n\nConfiguring this was not clearly documented in lib as would I expect. The two main changes we've done in our client were:\n- add the `WithDefaultServiceConfig` DialOption with the load balancing policy\n- specify a DNS URI pointing to the Headless Server we've mentioned\n\nThe result was like:\n```go\nimport (\n  \"google.golang.org/grpc\"\n)\n\nconn, err := grpc.Dial(\"dns:///my-headless-service:5000\",\n  grpc.WithDefaultServiceConfig(`{\"loadBalancingPolicy\":\"round_robin\"}`),\n)\n```\n\nSince I could not access the Headless Service from my local machine in development (it is an intra-cluster address), I had to set up a local DNS server to experiment with this, while on production I would not specify a DNS and let the Pod use its default.\n\nNever having worked with it before, the DNS URI tricked me into some errors. First, [the documentation](https://github.com/grpc/grpc/blob/master/doc/naming.md) states that the scheme is `dns:[//authority/]host[:port]`, which makes all slashes lookup optional when you do not specify an authority (the DNS server you want to use, if not the default configured on the OS), which was my case in production.\n\nSo while I was able to make this work locally by using `dns://localhost:1053/my-headless-service:5000`, in production I first tried a naked `dns:my-headless-service:5000` and a double-slashed `dns://my-headless-service:5000` before landing on the correct triple-slashed `dns:///my-headless-service:5000`.\n\nAlso, note that the port `5000` is not important in this resolution process. It is just the port where each pod is exposing my gRPC server.\n\n## Connection Timeout on gRPC server\n\nRemember my 3 constraints? The client configuration only solves number 1.\n\nIf I remove instances of my service, it would cause connections to fail and [the client to re-resolve the names](https://github.com/grpc/grpc-go/issues/3170#issuecomment-552517779). This is default behavior of the lib and solves constraint number 2.\n\nIf everything is stable, the client never re-resolves the names and recreates new connections. So, if I double the number of instances of my service, the new ones would never receive connections and would be idle. And this fails constraint 3.\n\nTo work around this, we've configured a [MAX_CONNECTION_AGE](https://github.com/grpc/proposal/blob/master/A9-server-side-conn-mgt.md):\n```go\nimport (\n  \"google.golang.org/grpc\"\n  \"google.golang.org/grpc/keepalive\"\n)\n\nopts := []grpc.ServerOption{\n  grpc.KeepaliveParams(keepalive.ServerParameters{\n  MaxConnectionAge: time.Minute * 5,\n  }),\n}\n```\n\nWhen the connection reaches its max-age, it will be closed and will trigger a re-resolve from the client. If new instances were added in the meantime, the client will see them now and open connections to them as well.\n\n## Demo\n\nI've set up [a demo](https://github.com/rafaeleyng/rafaeleyng.github.io/blob/dev/examples/grpc-load-balancing/README.md) with a local DNS server to show this working.\n","data":{"title":"gRPC load balancing with grpc-go","date":"2020-11-22T00:00:00.000Z","keywords":"grpc, load balancing, golang, kubernetes","excerpt":"A complete explanation of how to setup client-side gRPC load balancing using a DNS naming system.\n"}},"slug":"grpc-load-balancing-with-grpc-go"}}},"page":"/post","query":{"slug":"grpc-load-balancing-with-grpc-go"},"buildId":"E5zKtSWyHCoyngCKK54Qr","nextExport":true,"isFallback":false}</script><script nomodule="" src="/_next/static/runtime/polyfills-2998c20c30097f9c8a42.js"></script><script async="" data-next-page="/post" src="/_next/static/E5zKtSWyHCoyngCKK54Qr/pages/post.js"></script><script async="" data-next-page="/_app" src="/_next/static/E5zKtSWyHCoyngCKK54Qr/pages/_app.js"></script><script src="/_next/static/runtime/webpack-9369c5c69dbf6d4912cb.js" async=""></script><script src="/_next/static/chunks/framework.03885e3762b811c06d48.js" async=""></script><script src="/_next/static/chunks/commons.2d46a4062a63030a7155.js" async=""></script><script src="/_next/static/runtime/main-fd16d9fd7045bb52879a.js" async=""></script><script src="/_next/static/chunks/a1bc03cf.5784ec4ab76361fe4054.js" async=""></script><script src="/_next/static/chunks/e83c54905e1409242ee1a2330441cd232091e570.751c034d2cba01f871b4.js" async=""></script><script src="/_next/static/chunks/e83c54905e1409242ee1a2330441cd232091e570_CSS.c4a6afef92e8dd991da1.js" async=""></script><script src="/_next/static/E5zKtSWyHCoyngCKK54Qr/_buildManifest.js" async=""></script><script src="/_next/static/E5zKtSWyHCoyngCKK54Qr/_ssgManifest.js" async=""></script></body></html>